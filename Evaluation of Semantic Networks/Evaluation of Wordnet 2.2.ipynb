{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estonian Wordnet version 2.2\n",
    "\n",
    "Three path-based measures: path similarity, Leacock & Chodorow and Wu & Palmer are implemented on Estonian Wordnet version 2.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "from xml.dom import minidom\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllPaths(word, dic, pos, synsets):\n",
    "    \"\"\"\n",
    "    returns all the possible paths \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if word not in dic.keys():\n",
    "        return []\n",
    "    \n",
    "    senses = dic.get(word)\n",
    "    \n",
    "    if pos not in senses.keys():\n",
    "        return []\n",
    "    senses = senses.get(pos)\n",
    "    paths = []\n",
    "\n",
    "    for sense in senses.keys():\n",
    "        val = senses.get(sense)\n",
    "        path = getHypernyms(val,synsets)\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "def getHypernyms(val, synsets):\n",
    "    \"\"\"\n",
    "    returns all the hypernyms of a synset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    value = synsets.get(val)\n",
    "    path = []\n",
    "    path.append(val)\n",
    "    while True: \n",
    "        if 'hypernym' in value.keys():\n",
    "            hyp = value.get('hypernym')\n",
    "            path.append(hyp)\n",
    "            \n",
    "            if hyp in synsets.keys():\n",
    "                value = synsets.get(hyp)\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return path\n",
    "\n",
    "\n",
    "def shorthestPath(word1, word2, pos, dic, synsets):\n",
    "    \"\"\"\n",
    "    returns the shorthest path, path to root, paths to root, path length\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    p1 = getAllPaths(word1, dic, pos, synsets)\n",
    "    p2 = getAllPaths(word2, dic, pos, synsets)\n",
    "    if p1==[] or p2 == []:\n",
    "        return [[], [], []], [], [[], []], -1\n",
    "    shortest = []\n",
    "    p = []\n",
    "    toroot = []\n",
    "    s = 1000\n",
    "\n",
    "    for p_1 in p1:\n",
    "        for p_2 in p2: \n",
    "            if p_1[-1] == p_2[-1]:\n",
    "                path, to_root, l = getEdgeCount(p_1, p_2)\n",
    "\n",
    "                if l < s: \n",
    "                    s = l\n",
    "                    p = path\n",
    "                    toroot = to_root\n",
    "                    shortest = [p_1, p_2]\n",
    "\n",
    "            \n",
    "    if p == []:\n",
    "        p = [[], [], []]\n",
    "    if shortest == []:\n",
    "        shortest = [[], []]\n",
    "    return p, toroot, shortest, s\n",
    "\n",
    "def getEdgeCount(p1, p2):\n",
    "    \"\"\"\n",
    "    returns path between two synsets, path from lcs to root, path length\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pathp1 = []\n",
    "    pathp2 = []\n",
    "    lcs = []\n",
    "    lcs_root = []\n",
    "    count = 0\n",
    "    if p1 == p2: \n",
    "        return [[],  [p1[0]], []], p1, 0\n",
    "    if set(p1) < set(p2): #alamhulk \n",
    "        for p in p2: \n",
    "            if p not in p1: \n",
    "                pathp2.append(p) # this is lcs\n",
    "                count += 1\n",
    "            else:\n",
    "                if lcs == []:\n",
    "                    lcs.append(p) # this is lcs\n",
    "                lcs_root.append(p)\n",
    "    elif set(p2) < set(p1):\n",
    "        for p in p1: \n",
    "            if p not in p2: \n",
    "                pathp1.append(p)\n",
    "                count += 1\n",
    "            else:\n",
    "                if lcs == []:\n",
    "                    lcs.append(p) # this is lcs\n",
    "                lcs_root.append(p)\n",
    "        \n",
    "    else:\n",
    "        for p in p2: \n",
    "            if p not in p1: \n",
    "                pathp2.append(p)\n",
    "                count += 1\n",
    "            else:\n",
    "                if lcs == []:\n",
    "                    lcs.append(p) # this is lcs\n",
    "                lcs_root.append(p)\n",
    "        for p in p1: \n",
    "            if p not in p2: \n",
    "                pathp1.append(p)\n",
    "                count += 1\n",
    "    return [pathp1, lcs, pathp2], lcs_root, count\n",
    "        \n",
    "    \n",
    "def findDepth(dic, synsets):\n",
    "    \"\"\"\n",
    "    returns a dictionary containing max path lengths from root to leaf\n",
    "    \n",
    "    \"\"\"  \n",
    "    roots = []\n",
    "    long = {}\n",
    "    for key in dic.keys(): \n",
    "        vals = dic.get(key)\n",
    "        for pos in vals.keys():\n",
    "            senses = vals.get(pos)\n",
    "            for sense in senses.keys():\n",
    "                synset = senses.get(sense)\n",
    "                hypers = getHypernyms(synset, synsets)\n",
    "                if len(hypers) >0:\n",
    "                    root = hypers[-1]\n",
    "                    length = len(hypers)\n",
    "                    \n",
    "                    if root not in long.keys():\n",
    "                        long[root] = length\n",
    "                    else:\n",
    "                        if long.get(root) < length: \n",
    "                            long[root] = length\n",
    "    return long\n",
    "    \n",
    "def path_similarity(paths): \n",
    "    \"\"\"\n",
    "    returns path similarity, which is calculates as ps=1/min_path(synset1, synset2)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    l = len(paths[0]) + len(paths[1]) + len(paths[2])\n",
    "    if l == 0:\n",
    "        return -1\n",
    "                                            \n",
    "    return 1/l\n",
    "\n",
    "def lc(paths,depths, root): \n",
    "    \"\"\"\n",
    "    returns leacock & chodorow similarity, which is calculated as -log(path(synset1, synset2)/(2*depth_graph))\n",
    "    \n",
    "    \"\"\"\n",
    "    l = len(paths[0]) + len(paths[1]) + len(paths[2])\n",
    "    if len(root)==0:\n",
    "        return -1\n",
    "    depth = depths.get(root[-1])\n",
    "\n",
    "    if l == 0: \n",
    "        return -1\n",
    "    return -math.log(l/(2*depth))\n",
    "\n",
    "def wup(p, toroot):\n",
    "    \"\"\"\n",
    "    returns Wu & Palmer similarity, calculated as (2*depth_lcs)/(depth(synset1)+depth(synset2))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = len(p[0])\n",
    "    p2 = len(p[1])\n",
    "    \n",
    "    depth_lcs = len(toroot)\n",
    "    if (p1+p2) == 0:\n",
    "        return -1\n",
    "    #return (2*depth_lcs)/(p1+p2+2*depth_lcs)\n",
    "\n",
    "    return (2*(depth_lcs))/(p1+p2)\n",
    "\n",
    "def calc_results(df):\n",
    "    \"\"\"\n",
    "    returns dataframe containing correlations scores\n",
    "    \n",
    "    \"\"\"\n",
    "    results = pd.DataFrame(columns=[\"measure\", \"sim_set\", \"pearson\", \"spearman\", \"kendall\"]) \n",
    "    results = evaluate(df, results, \"ESL\", \"PS\")\n",
    "    results = evaluate(df, results, \"ESL\", \"LC\")\n",
    "    results = evaluate(df, results, \"ESL\", \"WUP\")\n",
    "    results = evaluate(df, results, \"SL\", \"PS\")\n",
    "    results = evaluate(df, results, \"SL\", \"LC\")\n",
    "    results = evaluate(df, results, \"SL\", \"WUP\")\n",
    "    return results\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate(df, results, sim_set, measure):\n",
    "    \"\"\"\n",
    "     calculates correlation coefficients\n",
    "    \n",
    "    \"\"\"\n",
    "    pearson = round(pearsonr(df[sim_set], df[measure])[0], 3)\n",
    "    spearman = round(spearmanr(df[sim_set], df[measure])[0], 3)\n",
    "    kendall = round(kendalltau(df[sim_set], df[measure])[0],3)\n",
    "    results = results.append({\"measure\":measure, \"sim_set\":sim_set, \"pearson\":pearson, \"spearman\":spearman, \"kendall\":kendall},\n",
    "                            ignore_index=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parsing the XML file\n",
    "\n",
    "Wordnet is in XML file, this file contains LexicalEntry and Synset elements. Dictionary containing synset ids (dic) is created and dictionary for getting synset entries (revers, which is not used in any method, just to understand path better) is created. Relations between synsets are from Synset elements.  \n",
    "<br>\n",
    "This XML can be downloaded from here https://gitlab.keeleressursid.ee/avalik/data/blob/master/estwn/estwn-et-2.2.0.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydoc = minidom.parse('estwn-et-2.2.0.xml')\n",
    "items = mydoc.getElementsByTagName(\"LexicalEntry\")\n",
    "dic = defaultdict(dict)\n",
    "revers = defaultdict(str)\n",
    "\n",
    "for item in items:\n",
    "    writtenForm = item.getElementsByTagName('Lemma')[0].attributes['writtenForm'].value\n",
    "    POS = item.getElementsByTagName('Lemma')[0].attributes['partOfSpeech'].value\n",
    "    senses = {}\n",
    "    for s in item.getElementsByTagName('Sense'):\n",
    "        sid = s.attributes['id'].value\n",
    "        synset = s.attributes['synset'].value\n",
    "        \n",
    "        senses[sid] = synset\n",
    "        if synset not in revers.keys():\n",
    "            revers[synset] = [sid]\n",
    "        else:\n",
    "            r = revers.get(synset)\n",
    "            r.append(sid)\n",
    "            revers[synset] = r\n",
    "    dic[writtenForm][POS] = senses\n",
    "    \n",
    "    \n",
    "synsets = defaultdict(dict)\n",
    "syns = mydoc.getElementsByTagName(\"Synset\")\n",
    "for item in syns:\n",
    "    sid = item.attributes['id'].value\n",
    "    rels = {}\n",
    "    for s in item.getElementsByTagName('SynsetRelation'):\n",
    "        rel = s.attributes['relType'].value\n",
    "        target = s.attributes['target'].value\n",
    "        rels[rel] = target\n",
    "    synsets[sid] = rels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarities and correlations\n",
    "First, necessary variables are initiated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Ratings.xlsx\")\n",
    "depths = findDepth(dic, synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity between words from the EstSimLex-999 and correlation with human score is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity_scores = pd.DataFrame(columns=[\"sõna1\", \"sõna2\",\"PS\", \"LC\", \"WUP\", \"ESL\", \"SL\"])\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    s1 = row[\"sõna 1\"]\n",
    "    s2 = row[\"sõna 2\"]\n",
    "\n",
    "    pos = row[\"POS\"].lower()\n",
    "    paths,toroot,s,e = shorthestPath(s1, s2,row[\"POS\"].lower(),dic, synsets)\n",
    "    path_sim = path_similarity(paths)\n",
    "    l = lc(paths, depths, toroot ) \n",
    "    w = wup(s,toroot)\n",
    "    sl =row[\"SimLex999\"]\n",
    "    esl = row[\"Average\"]\n",
    "    if path_sim != -1:\n",
    "        similarity_scores = similarity_scores.append({\"sõna1\":s1, \"sõna2\":s2, \"PS\":path_sim, \"LC\":l, \"WUP\":w, \"ESL\":esl,\n",
    "                                                          \"SL\":sl}, ignore_index=True)\n",
    "        \n",
    "results = calc_results(similarity_scores)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_excel(\"wordnet_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
