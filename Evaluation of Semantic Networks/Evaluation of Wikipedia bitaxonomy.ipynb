{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia bitaxonomy\n",
    "\n",
    "In this notebook, three path-based measures (path similarity, Leacock & Chodorow, Wu & Palmer) for Wikipedia bitaxonomy. \n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rpages = {}\n",
    "for p in pages.keys():\n",
    "    p1 = pages.get(p)\n",
    "    for page in p1: \n",
    "        if 'HYPERNYM' in page.keys():\n",
    "            hyps = page.get('HYPERNYM')\n",
    "            for h in hyps:\n",
    "                if h in rpages.keys():\n",
    "                    val = rpages.get(h)\n",
    "                    val.append(p)\n",
    "                    rpages[h] = val\n",
    "                else:\n",
    "                    rpages[h]=[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcEdges(s1, s2): \n",
    "    \"\"\"\n",
    "    returns edge count of the path \n",
    "    \n",
    "    \"\"\"\n",
    "    if s1[-1] == s2[-1]:\n",
    "        if set(s1) == set(s2): # two nodes have the same hypernym, so 2 edges in total / \\\n",
    "            return 2\n",
    "        \n",
    "        if set(s1).issubset(set(s2)): # if one is part of the other\n",
    "            return len(s2) - len(s1) + 2\n",
    "        \n",
    "        if set(s2).issubset(set(s1)): # same logic as the previous\n",
    "            return len(s1)-len(s2) +2\n",
    "        \n",
    "        c = set(s1).intersection(set(s2)) # finding intersection of those lists\n",
    "        l1 = len(s1) - len(c) + 1 \n",
    "        l2 = len(s2) - len(c) + 1\n",
    "        return l1+l2\n",
    "    \n",
    "    return 1000 # if no intersection between those paths were found, return 1000\n",
    "     \n",
    "    \n",
    "def calcNodes(s1, s2): \n",
    "    \"\"\"\n",
    "    returns nodes count on the path\n",
    "    \n",
    "    \"\"\"\n",
    "    return calcEdges(s1, s2) + 1\n",
    "\n",
    "\n",
    "def PS(s1, s2): \n",
    "    \"\"\" \n",
    "    path similarity, calculated as ps=1/(minPath(s1,s2)+1)\n",
    "    \n",
    "    \"\"\" \n",
    "    nodes = calcNodes(s1, s2)\n",
    "\n",
    "    ps = 1/nodes\n",
    "    return ps\n",
    "\n",
    "\n",
    "def LC(s1, s2, depth): \n",
    "    \"\"\"\n",
    "    Leacock & Chodorow, calculated as -log(minPath(s1, s2)/(2*(max_graph_depth)))\n",
    "    \n",
    "    \"\"\"\n",
    "    l = calcNodes(s1, s2)\n",
    "\n",
    "    lc = -math.log(l/(2*depth))\n",
    "    return lc\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    \"\"\"\n",
    "    returns the first same value from 2 lists\n",
    "    \n",
    "    \"\"\"\n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "\n",
    "\n",
    "def depth(lcs, s):\n",
    "    \"\"\"\n",
    "    returns the depth (from root to some node)\n",
    "    \n",
    "    \"\"\"\n",
    "    i = s.index(lcs)\n",
    "    slice_s = s[i:]\n",
    "    return len(slice_s)\n",
    "\n",
    "\n",
    "def length(s1, s2): \n",
    "    lst = len(intersection(s1, s2))\n",
    " \n",
    "    l1 = s1[:-lst]\n",
    "    l2 = s2[:-lst]\n",
    "    return len(l1)+1, len(l2)+2\n",
    "    \n",
    "    \n",
    "def lcs(s1, s2):\n",
    "    \"\"\"\n",
    "    returns the least common subsumer \n",
    "    \n",
    "    \"\"\"\n",
    "    lst = intersection(s1, s2)\n",
    "    return lst[0]\n",
    "    \n",
    "    \n",
    "def WUP(s1, s2):\n",
    "    \"\"\"\n",
    "    Wu & Palmer, calculated as ( 2*depth(lcs(s1,s2)) ) / ( depth(s1) + depth(s2) )\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return (2*depth(lcs(s1, s2), s1)/(length(s1, s2)[0] + length(s1, s2)[1]+ 2*depth(lcs(s1, s2), s1)) )\n",
    "\n",
    "\n",
    "def findDepth(categories, pages):\n",
    "    \"\"\"\n",
    "    returns the dictionary, where the maximum path length from root to leaf in category taxonomy is saved\n",
    "    \n",
    "    \"\"\"\n",
    "    longs = {}\n",
    "    \n",
    "    for l in pages.keys():\n",
    "        all_path = all_paths(categories, pages, l)\n",
    "        \n",
    "        for p in all_path:\n",
    "            length = len(p)\n",
    "            root = p[-1]\n",
    "            \n",
    "            if root not in longs.keys():\n",
    "                longs[root] = length\n",
    "            else: \n",
    "                if longs.get(root) < length: \n",
    "                    longs[root] = length\n",
    "    return longs\n",
    "\n",
    "\n",
    "def all_paths(categories, pages, w1):\n",
    "    \"\"\"\n",
    "    returns all possible paths to a root node (first node is the page node, using cross_links it is moved to category taxonomy)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    visited = []\n",
    "    paths = []\n",
    "    \n",
    "    if w1 in pages.keys():        \n",
    "        cs = [ link for page in  pages.get(w1) if 'CROSS_LINK' in page.keys() for link in page.get('CROSS_LINK')]\n",
    "    else: \n",
    "        return []\n",
    "    queue = [[w] for w in cs]\n",
    "    \n",
    "    while queue: \n",
    "        path = queue.pop(0)\n",
    "        val = path[-1]\n",
    "        neighbours = getAllEdges(val, categories)\n",
    "        \n",
    "        if neighbours == set():\n",
    "            paths.append(path)\n",
    "            \n",
    "        for n in neighbours:\n",
    "            new = list(path)\n",
    "            new.append(n)\n",
    "            queue.append(new)\n",
    "    return [p for p in paths if p != [w1]]\n",
    "\n",
    "\n",
    "def all_paths_pages(pages, w1):\n",
    "    \"\"\"\n",
    "    returns all possible paths to a root node, only using page taxonomy\n",
    "    \n",
    "    \"\"\"\n",
    "    visited = []\n",
    "    paths = []\n",
    "    queue = [[w1]]\n",
    "    \n",
    "    while queue: \n",
    "        path = queue.pop(0)\n",
    "        val = path[-1]\n",
    "        neighbours = getAllEdges(val, pages)\n",
    "        \n",
    "        if neighbours == set():\n",
    "            paths.append(path)\n",
    "            \n",
    "        for n in neighbours:\n",
    "            new = list(path)\n",
    "            new.append(n)\n",
    "            queue.append(new)\n",
    "            \n",
    "    return [p for p in paths if p != [w1]]\n",
    "\n",
    "\n",
    "def minPath(h1, h2):\n",
    "    \"\"\"\n",
    "    returns the paths to root, with what minimum path between two nodes is found\n",
    "    \n",
    "    \"\"\"\n",
    "    shortest = 1000\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    for h1_path in h1: \n",
    "        for h2_path in h2: \n",
    "            path_len= calcEdges(h1_path, h2_path)\n",
    "            if path_len < shortest: \n",
    "                shortest = path_len\n",
    "                p1 = h1_path\n",
    "                p2 = h2_path\n",
    " \n",
    "    if shortest == 1000: \n",
    "        return -1, -1\n",
    "    return p1, p2\n",
    "\n",
    "\n",
    "def findDepthPages(pages):\n",
    "    \"\"\"\n",
    "    returns the maximum path lengths from root to leaf in page taxonomy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    longs = {}\n",
    "    for l in pages.keys():\n",
    "        all_path = all_paths_pages(pages, l)\n",
    "        for p in all_path:\n",
    "            length = len(p)\n",
    "            root = p[-1]\n",
    "            if root not in longs.keys():\n",
    "                longs[root] = length\n",
    "            else: \n",
    "                if longs.get(root) < length: \n",
    "                    longs[root] = length\n",
    "    return longs\n",
    "\n",
    "\n",
    "def getAllEdges(w, dic):\n",
    "    \"\"\"\n",
    "    returns the hypernyms of a node\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    edges = []\n",
    "    if w in dic.keys():\n",
    "        val = dic.get(w)\n",
    "        for v in val:\n",
    "            if 'HYPERNYM' in v.keys():\n",
    "                hyp = v.get('HYPERNYM')\n",
    "                edges.extend(hyp)\n",
    "    return set(edges)\n",
    "\n",
    "def calc_results(df):\n",
    "    \"\"\"\n",
    "    returns dataframe containing correlations scores\n",
    "    \n",
    "    \"\"\"\n",
    "    results = pd.DataFrame(columns=[\"measure\", \"sim_set\", \"pearson\", \"spearman\", \"kendall\"]) \n",
    "    results = evaluate(df, results, \"ESL\", \"PS\")\n",
    "    results = evaluate(df, results, \"ESL\", \"LC\")\n",
    "    results = evaluate(df, results, \"ESL\", \"WUP\")\n",
    "    results = evaluate(df, results, \"SL\", \"PS\")\n",
    "    results = evaluate(df, results, \"SL\", \"LC\")\n",
    "    results = evaluate(df, results, \"SL\", \"WUP\")\n",
    "    return results\n",
    "    \n",
    "    \n",
    "    \n",
    "def evaluate(df, results, sim_set, measure):\n",
    "    \"\"\"\n",
    "     calculates correlation coefficients\n",
    "    \n",
    "    \"\"\"\n",
    "    pearson = round(pearsonr(df[sim_set], df[measure])[0], 3)\n",
    "    spearman = round(spearmanr(df[sim_set], df[measure])[0], 3)\n",
    "    kendall = round(kendalltau(df[sim_set], df[measure])[0],3)\n",
    "    results = results.append({\"measure\":measure, \"sim_set\":sim_set, \"pearson\":pearson, \"spearman\":spearman, \"kendall\":kendall},\n",
    "                            ignore_index=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML file parsing\n",
    "\n",
    "Bitaxonomy is in a XML file. This file contains doc elements, each doc element is a Wikipedia page or category. Hypernyms are defined for each group. Also for every page, there a so called cross_links to wikipedia categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parsing the XML file \n",
    "mydoc = minidom.parse('ET_taxonomy.xml')\n",
    "items = mydoc.getElementsByTagName('doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seperating pages and categories \n",
    "pages = {}\n",
    "categories = {}\n",
    "alls = {}\n",
    "for item in items: \n",
    "    name = \"\"\n",
    "    fields = item.getElementsByTagName('field')\n",
    "    fieldDict = {}\n",
    "    for field in fields: \n",
    "        fname = field.attributes['name'].value # PAGE/CATEGORY/HYPERNYM/HYPERLINK\n",
    "        vals = field.getElementsByTagName('val') #<val>\n",
    "        values = []\n",
    "        for val in vals: \n",
    "            value = val.firstChild.data.strip().lower()\n",
    "            \n",
    "            if fname == \"PAGE\" or fname == \"CATEGORY\":\n",
    "                fieldDict[\"FNAME\"] = fname \n",
    "                name = value\n",
    "            else:\n",
    "                values.append(value)\n",
    "                \n",
    "        if values != []:\n",
    "            fieldDict[fname] = values\n",
    "            \n",
    "    if len(fieldDict.keys()) != 1: \n",
    "        if name in alls.keys(): \n",
    "            d = alls.get(name)\n",
    "            d.append(fieldDict)\n",
    "            alls[name] = d\n",
    "        else:\n",
    "            alls[name] = [fieldDict]\n",
    "            \n",
    "    if name in categories.keys() and fieldDict[\"FNAME\"]==\"CATEGORY\":\n",
    "        d = categories.get(name)\n",
    "        d.append(fieldDict)\n",
    "        categories[name] = d\n",
    "    elif name in pages.keys() and fieldDict[\"FNAME\"]==\"PAGE\":\n",
    "        d = pages.get(name)\n",
    "        d.append(fieldDict)\n",
    "        pages[name]=d\n",
    "    \n",
    "    else:\n",
    "        if fieldDict[\"FNAME\"] == \"CATEGORY\":\n",
    "            categories[name] = [fieldDict]\n",
    "        elif fieldDict[\"FNAME\"] == \"PAGE\":\n",
    "            pages[name]=[fieldDict]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity and correlations\n",
    "First, similarity is calculated beween the EstSimLex-999 pairs and then the correlations between those and humans scores are calculated. \n",
    "<br>\n",
    "First, the needed data files are loaded and some variables are initiated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the EstSimLex-999 data set\n",
    "file_name = \"Ratings.xlsx\"\n",
    "data = pd.read_excel(file_name)\n",
    "# finding max depths of  page taxonomy\n",
    "depths_graph_pages = findDepthPages(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page taxonomy is used to calculate similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe, where the similarity scores will be saved\n",
    "similarity_scores_pages = pd.DataFrame(columns=[\"sõna1\", \"sõna2\",\"PS\", \"LC\", \"WUP\", \"ESL\", \"SL\"])\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    w1 = row[\"sõna 1\"]\n",
    "    w2 = row[\"sõna 2\"]\n",
    "    hyp1 = all_paths_pages(pages, w1)\n",
    "    hyp2 = all_paths_pages(pages, w2)\n",
    "    esl = row[\"Average\"]\n",
    "    sl = row[\"SimLex999\"]\n",
    "    if hyp1 != [] and hyp2 != []:\n",
    "        p1,p2= minPath(hyp1, hyp2)\n",
    "        \n",
    "        if p1 != -1:\n",
    "            path_sim = PS(p1, p2)\n",
    "            d = depths_graph_pages.get(p1[-1])+1\n",
    "            l = LC(p1, p2, d)\n",
    "            w = WUP(p1, p2)\n",
    "            similarity_scores_pages = similarity_scores_pages.append({\"sõna1\":w1, \"sõna2\":w2, \"PS\":path_sim, \"LC\":l, \"WUP\":w, \"ESL\":esl,\n",
    "                                                          \"SL\":sl}, ignore_index=True)\n",
    "            \n",
    "\n",
    "\n",
    "results_page = calc_results(similarity_scores_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving results to excel\n",
    "results_page.to_excel(\"pages_tax_results.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
